# 1.  Data Description
The “Bank Marketing” data is related with direct marketing campaigns of a Portuguese banking
institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client
was required, to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. We use the
“bank-full.csv” with 16 attributes and 45212 instances ordered by date. There are 6 continuous attributes and 10
categorical attributes. We normalize and standardize all the values to the same scale and convert the values of
categorical attributes such as “male” or “female” to numerical values of “0” or “1”. The classification goal is to
predict if the client will subscribe a term deposit or not; in the other word, y is “yes” or “no”.

# 2. Design of Experiment
## 2.1 Level of balance
Imbalanced dataset happens when classes values are not represented equally. The “Bank Marketing Dataset” is an imbalanced dataset with 4329 instances with target value “yes” and 40883 instances with target value of “no”; the ratio of “no” and “yes” instances is 10:1 ratio. To investigate how the performance metrics change as the level of balance in the data change, we use the Synthetic Minority Over-sampling Technique which is an oversampling method creating synthetic samples from the minor class instead of creating duplicates. We would run experiments on five ratios of positive over negative instances which are 0.2, 0.4, 0.6, 0.8, 1.0 by gradually increasing the number of positive instances.
## 2.2 Sample size
To investigate how sensitive different datasets and different algorithm respond to sample size, we random sample the two datasets. The “Bank Marketing Dataset” has 45212 instances and we random sample it into 10 datasets with the sample size of 50, 100, 500, 1000, 3000, 5000, 8000, 10000, 30000 and 45212. We would be interested to observed how multiple performance metrics react to different sample sizes from the same dataset.
## 2.3 Feature size
We use the Principle Component Analysis (PCA) on 5 different number of components 3, 5, 7, 10, 12 to investigate how the performance metrics change as the feature sizes change

# 3. Classification Algorithm
## 3.1 Decision Tree
C4.5 use post-pruning technique which going backward to a completed tree and replace the branches that don’t satisfy the pruning conditions with leaf nodes. The pruning conditions in J48 algorithm combines both statistical pruning and minimum-description length pruning techniques. The statistical pruning would replace any branches with Chi-square statistics smaller than the deviation Dev(X) which is a measure of deviation of the data from complete absence of pattern.

![alt text](https://user-images.githubusercontent.com/30711638/35753004-0e2ffbbc-082c-11e8-9626-73c6b569ced7.png)

Figure 1 shows that when the ratio of imbalance ranging from 0.2 to 0.6, the accuracy and precision decreases quickly. This is expected when the data is severely imbalanced such as when the ratio of positive over negative instances is 0.2, the learner can classify most instances as negative and still get a minimum accuracy of 0.8. This is proved in figure 2 where only 6% of negative instances misclassified yet 46% positive instances misclassified. As the dataset becomes more balanced, the classifier start to compensate the accuracy of negative class for the accuracy of positive class, dragging the overall accuracy down. The percent of negative instances misclassified keep increasing for the imbalance ratio from 0.2 to 0.6 because the total number of negative instances is increasing. However, at the imbalance ratio of 0.8, we can observe the recall which indicates false negatives decrease significantly and precision which indicates positives increase significantly. When the negatives and positives become completely balanced, all accuracy, precision and recall converge to one value. 

![alt text](https://user-images.githubusercontent.com/30711638/35753192-afcbf480-082c-11e8-8944-9335b0572fed.png)

Figure 3 shows that recall indicating false negatives is always higher than precision indicating false positives because there is more negatives than positives. It seems that the performance metrics on this dataset don’t vary much as the sample sizes change.

![alt text](https://user-images.githubusercontent.com/30711638/35753347-66ea419e-082d-11e8-9e6f-0d8ca13cd36c.png)

Figure 4 shows that the decision tree would perform the best with the 10 most significant features in data. The tree would perform worst will lack of information from important attributes. On the other hand, with more than 10 features, the tree starts building on redundant attributes and the accuracy would decrease. C4.5 algorithm seek to make optimal splits in attribute values and those attributes that are more correlated with the prediction are split on first. As the tree grows deeper, attributes used for prediction become less relevant and this may only be beneficial by chance in the training dataset. Excessive feature size is more likely to encounter multicollinearity which happens when two variables both explain the same thing, a decision tree will only choose the best one while most other algorithms would choose both attributes. Also, it might increase variance because our model starts to fit noise of training data, causing overfitting problem. In short, the decision tree algorithm is relatively sensitive to the feature size and the relevance of attributes in training data.

## 3.2 Boosting
![alt text](https://user-images.githubusercontent.com/30711638/35753416-ad4f753c-082d-11e8-9344-917398b39f7e.png)
Figure 5 shows that test and train error of Real and Discrete AdaBoost have lower error than the weak learners of Decision Stump and Decision Tree. Real AdaBoost has train error decreasing exponentially as the number of estimators increase because with more learners, boosting learners give gradually larger weights to the data that are last misclassified, our algorithm become better at predicting misclassified data and error decreasing quickly. Test error slightly decrease during the first 200 estimators and fluctuating up and down during the last 200 estimators. The Discrete AdaBoost’s train errors and test errors are quite stable and it performs worse than Real AdaBoost for our dataset.

## 3.3	K-nearest neighbors
![alt text](https://user-images.githubusercontent.com/30711638/35754646-247d10f2-0832-11e8-88c4-7f5156deba60.png)
As the number of neighbors needed to form decision boundary increases, our decision boundary become smoother. Figure 6 shows that the error rate at K=1 is always zero for the training sample. This is because the closest point to any training point is itself and the prediction is always accurate. However, the test error is the highest at K=1 because our model is overfitting the boundaries. The error rate initially decreases and reaches a minima at K=2. After the minima, test error increases as K increasing. Table 1 confirms that at K=2, the error of 10 folds in cross validations are almost the highest comparing to other K values. One thing to note is that the last folder which is the last 10% of the data has the lowest accuracy scores for all K values. This might be because this part of the data contains a lot of outliners.

## 3.4	Neural Networks
![alt text](https://user-images.githubusercontent.com/30711638/35754752-9d68f9b8-0832-11e8-95f9-3ad5838d481a.png)
Table 2 shows that with 5000 instances, accuracy, precision and recall gradually increase as the hidden layer sizes and number of nodes each layer increase. However, accuracy slightly decrease when the hidden layer size reaches to 5. It means that for 5000 instance, 4 layers is the optimal number of layer that can minimize the error and design a trained network that generalize well. When the layer size reaches above the optimal number, our Neural Network model will overfit to the training data and the accuracy starts decreasing. The optimal number of hidden layers and nodes depend on the number of input and output nodes, amount of training data available, the complexity of the function that is trying to be learned and the training algorithm. Table 3 shows that all 3 performance metrics keep increasing to the layer size of five because we have more input nodes and our dataset requires a more complicated neural network with more hidden layers. Also, we can observe that if the network only has 1 or 2 layers, a decrease from 500 nodes to 50 nodes result a small decrease in in accuracy and big decrease in precision and recall. It means the network has too few nodes, it will lead to under-fitting because high bias resulting from the fact that a small number of nodes try to capture the complexity of the predictive factors.

![alt text](https://user-images.githubusercontent.com/30711638/35754873-0fad3f0c-0833-11e8-8a82-fb933692b3f4.png)

Figure 7 shows that for the sample sizes ranging from 50 to 3000, the accuracy is perfectly 100%. This is a sign that this neural network with layer size of 5, node number of 500, Adam solver and Relu activation function, the training set with 50 to 3000 instances would overfit our data. It fits every training data exactly and fail to predict new data from testing set correctly. That’s why the train MSE is 0 and test MSE is high. However, when the sample sizes that are bigger than 3000, the network starts to generalize better, misclassifying some training data points yet could predict the testing data more correctly. That’s why train MSE starts increasing and test MSE decrease. The ratio of training data and testing data is 80% to 20%, so the error from training data has more weights than that of testing data and ultimately dragging the overall accuracy, precision, recall of the model down. In this case, a low accuracy doesn’t indicate a bad model and we might choose a model with low accuracy but less overfit our training data.
![alt text](https://user-images.githubusercontent.com/30711638/35754955-6136b6c8-0833-11e8-8432-bb9d282c27f4.png)
After experimenting with many sample sizes, it reveals that the neural network performs the best with ‘relu’ and ‘tanh’ activation function and ‘identity’ and ‘logistic’ function don’t perform very well for our dataset as shown in figure 9. Figure 10 shows that the neural network performs well with ‘adam’ solver; the next best solver is ‘lbfgs’ and the worst solver is ‘sgd’. This pattern is true for all sample sizes that we experimented. 

## 3.5	Support Vector Machines (SVM)
We can observe that higher gamma result higher accuracy because it fit the training data more closely. 

![alt text](https://user-images.githubusercontent.com/30711638/35754997-962cb6f2-0833-11e8-8b98-e0587e1abd05.png)

Figure 11, 12, 13 show that accuracy is unchanged with Linear kernel and as penalty parameter C increases exponentially with the rate of 2n, the accuracy of Poly kernel also increases exponentially yet Rbf kernel increases with slower rate. “C” is penalty parameter of the error term. It also controls the trade-off between smooth decision boundary and classifying the training points correctly. More specifically, the higher the value of C, the closer the model tries to fit training data with lower error, yet the less smooth the decision boundary and over-fitting is more likely to happen. It seems that our data achieve highest accuracy for most C with ‘poly’ kernel, ‘rbf’ kernel is the second best and ‘linear’ kernel perform the worst. It means that our decision boundary of our SVM is rather a non-linear than linear hyperplane 

![alt text](https://user-images.githubusercontent.com/30711638/35755034-b7e4ce06-0833-11e8-99c4-9e1f0e27e658.png)

Similar to the neural network, the accuracy, precision, recall of model slightly decline as the sample size increases. Except for linear kernel where the sample size is performance metrics are unchanged as the sample size increase, SVM with ‘rbf’ and ‘poly’ kernel has the rate of accuracy decreasing much faster than that of ANN. It can be interpreted that for our dataset, SVM is more sensitive to overfitting and changes in sample size than ANN. 
![alt text](https://user-images.githubusercontent.com/30711638/35755111-fbf9e450-0833-11e8-9188-563ab4ace16a.png)

Figure 17 shows that as gamma increases, the accuracy, precision, and recall increase. Also note that when gamma is too small, especially below 0.001, the values of precision and recall reaches to 0 because the solver must terminate early. SVM uses convex optimization to find the global optima and gamma is kernel coefficient for ‘rbf’ and ‘poly’. The higher the value of gamma, the smaller the step size in each iteration to find the global optima is. The advantage of this is that it would optima value found would be closer to real optima, but it’s also more likely we get trapped into local optima and with finite number of iterations that we specified and terminated before the algorithm can find the true values, setting the precision and recall to the default value of 0. When gamma is too big such as above 10, the training time, testing time and generalization error increase, causing over-fitting problem. 
# Conclusion and Comparision
We can see that with a dataset small feature size as “Bank Marketing Dataset” can perform very well with simple algorithm such as decision tree, boosting and kNN. We can notice that based on the splitting-attribute algorithm, decision tree can perform very well with the highest accuracy of 91.9% even though the dataset is imbalanced. However, decision tree can be very sensitive to the feature size and we should not use it when we have the feature size that is bigger than sample size. Also, decision tree normally perform much better with some dimensionality reduction. Moreover, we can always use ensemble techniques such as boosting to increase the performance of simple classifier as decision tree. It is shown in our experiment above that boosting can significantly decrease training and testing error, making a weak learner become a more robust learner. KNN performs well with this dataset with the highest accuracy around 89% achieved with the optimal K = 2. We learn that we can graph the train and test error of KNN to find the optimal value of K. With a dataset with small number of feature size like this, ANN and SVM appears to be somewhat of an overshoot model with many instances that their models overfit our data. We can use SVM and ANN for these kind of dataset, but we just have to be careful to check the train and test error, compare accuracy, precision and recall and use cross validation to avoid overfitting. The highest accuracy ANN achieved is 97.7% and one of SVM is 93.5%. However, we haven’t test these 2 algorithms against the imbalance of dataset, so this accuracy might be misinterpreted. 






